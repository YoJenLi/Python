{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Gensim Word2Vec模型訓練維基數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.维基百科-條目文本整合下載\n",
    " - 先下載維基百科條目文本\n",
    "https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2\n",
    " - 建立Word2Vec 資料夾，待會用到的檔案都放這邊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.安裝套件\n",
    " - pip install gensim \n",
    " - 確定你的Python 有裝 cython 沒有的話就 pip install cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.數據提取\n",
    " - 首先需要從壓縮包中提取出中文維基的條目文本。在Word2Vec路徑下建立process_wiki.py文件\n",
    " - cmd 切換到Word2Vec路徑 指令碼 python process_wiki.py zhwiki-latest-pages-articles.xml.bz2 wiki.zh.text \n",
    " - 執行完會多個 wiki.zh.text\n",
    " - process_wiki.py 如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    " \n",
    "from gensim.corpora import WikiCorpus\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    " \n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    " \n",
    "    # check and process input arguments\n",
    "    if len(sys.argv) < 3:\n",
    "        print (globals()['__doc__'] % locals())\n",
    "        sys.exit(1)\n",
    "    inp, outp = sys.argv[1:3]\n",
    "    space = \" \"\n",
    "    i = 0\n",
    " \n",
    "    output = open(outp, 'w')\n",
    "    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(space.join(text) + \"\\n\")\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            logger.info(\"Saved \" + str(i) + \" articles\")\n",
    " \n",
    "    output.close()\n",
    "    logger.info(\"Finished Saved \" + str(i) + \" articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.使用結巴進行中文分詞\n",
    " - 建議使用繁體字典，跑出來的結果會比較偏向常用詞\n",
    " - 這邊大概要跑1-2小時，看資料量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import sys \n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "jieba.load_userdict(\"dict_keyw.txt\")     \n",
    "jieba.load_userdict(\"dict_cbdic.txt\")\n",
    "\n",
    " \n",
    "fr = open('wiki.zh.text', 'r')\n",
    "fw = open('wiki.zh.word.text', 'w')\n",
    " \n",
    "count = 0\n",
    "for line in fr:\n",
    "    count += 1\n",
    "    print count \n",
    " \n",
    "    result = ''\n",
    "    line = line.rstrip('\\n').split()\n",
    "    for item in line:\n",
    "        item = jieba.cut(item)\n",
    "        for i in item:\n",
    "            result += i + ' '\n",
    "    fw.write(result[:-1] + '\\n')\n",
    " \n",
    "fr.close()\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.模型訓練\n",
    " - 使用gensim包提供的word2vec模型進行訓練\n",
    " - 指令碼：python train_word2vec_model.py wiki.zh.word.text wiki.zh.text.model wiki.zh.text.vector \n",
    " - 訓練完畢會有四個檔 wiki.zh.text.model、wiki.zh.text.model.syn0.npy、wiki.zh.text.model.syn1.npy、wiki.zh.text.vecto\n",
    " - 模型訓練請用python3執行，剛開始使用py2速度慢到會讓你想死。\n",
    " - train_word2vec_model.py如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import multiprocessing\n",
    " \n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    " \n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    " \n",
    "    # check and process input arguments\n",
    "    if len(sys.argv) < 4:\n",
    "        print (globals()['__doc__'] % locals())\n",
    "        sys.exit(1)\n",
    "    inp, outp1, outp2 = sys.argv[1:4]\n",
    " \n",
    "    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5,\n",
    "            workers=multiprocessing.cpu_count())\n",
    " \n",
    "    # trim unneeded model memory = use(much) less RAM\n",
    "    #model.init_sims(replace=True)\n",
    "    model.save(outp1)\n",
    "    model.save_word2vec_format(outp2, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.模型使用\n",
    " - 上面四個檔要放一起\n",
    " - most_similar() 給定詞語遍歷其他所有詞語並返回與其相關度最高的10個詞語\n",
    " - similarity() 返回兩個詞語的相關度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load(\"wiki.zh.text.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "足球运动 0.5966246724128723\n",
      "冰球 0.5476202368736267\n",
      "排球 0.5415818691253662\n",
      "手球 0.527426540851593\n",
      "板球 0.5252264738082886\n",
      "美式足球 0.5022620558738708\n",
      "英超球 0.49216344952583313\n",
      "踢球 0.49188557267189026\n",
      "世界足球 0.48957183957099915\n",
      "棒球 0.48686718940734863\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar('足球')\n",
    "for ele in result:\n",
    "    print(ele[0],ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757236245216\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('男人','女人'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蟑螂 0.6209989190101624\n",
      "烏龜 0.6205072402954102\n",
      "螃蟹 0.612657904624939\n",
      "老鼠 0.5995995402336121\n",
      "蚱蜢 0.5932121276855469\n",
      "小狗 0.5863862037658691\n",
      "蜥蜴 0.585473895072937\n",
      "猩猩 0.5825787782669067\n",
      "巫婆 0.5785835981369019\n",
      "狐狸 0.5750727653503418\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar('青蛙')\n",
    "for ele in result:\n",
    "    print(ele[0],ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "男人 0.757236123085022\n",
      "家伙 0.520851731300354\n",
      "傻瓜 0.514291524887085\n",
      "陌生人 0.49404311180114746\n",
      "老公 0.4916646182537079\n",
      "女孩 0.48187896609306335\n",
      "女孩子 0.4746483862400055\n",
      "老婆 0.46345457434654236\n",
      "女明星 0.4593360424041748\n",
      "撒嬌 0.45863184332847595\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar('女人')\n",
    "for ele in result:\n",
    "    print(ele[0],ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鞋子 0.7607700228691101\n",
      "衣物 0.7605881094932556\n",
      "裙子 0.717649519443512\n",
      "大衣 0.6882219910621643\n",
      "外衣 0.6878334283828735\n",
      "外套 0.6821765899658203\n",
      "內褲 0.6808929443359375\n",
      "褲子 0.6739594340324402\n",
      "上衣 0.6662149429321289\n",
      "背心 0.6620113849639893\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar('衣服')\n",
    "for ele in result:\n",
    "    print(ele[0],ele[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參考資料  \n",
    "#### http://zhanghonglun.cn/blog/\n",
    "#### http://www.52nlp.cn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
